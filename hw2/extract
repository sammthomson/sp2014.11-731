#!/usr/bin/env python
import argparse
import json
# import string
import sys
from stemmer.czech_stemmer import cz_stem
# from kenlm import LanguageModel
from unidecode import unidecode
# import re

# PREFIX_LEN = 3
# LM = LanguageModel('data/ptb/czech_pdt_train.aggressive.stemmed.binary')
# AGGRO_LM = LanguageModel('data/europarl/europarl-v7.cs.aggressive.stemmed.binary')
# LM = LanguageModel('data/europarl/europarl-v7.cs.binary')
# VOWELS = re.compile('[aeiouy]')


def harmonic_mean(*xs):
    xs = list(xs)
    return len(xs) / sum(1.0 / x for x in xs) if min(xs) > 0 else 0.0


def prf(a, b, smoothing=0.0):
    precision = smoothing + (1-smoothing) * sum(1 for word in a if word in b) / float(len(a))
    recall = smoothing + (1-smoothing) * sum(1 for word in b if word in a) / float(len(b))
    return precision, recall, harmonic_mean(precision, recall)


def ngrams(l, n=3):
    return [zip(*[l[i:] for i in range(n)])]


def stem_all(words):
    return [cz_stem(word) for word in words]


def normalize(hyp):
    hyp = unidecode(hyp.lower().decode('utf8'))
    # hyp = hyp.strip(string.punctuation)
    # hyp = VOWELS.sub("a", hyp)
    return hyp


def extract_features(hyp, ref):
    hypothesis_words = normalize(hyp).split()
    reference_words = normalize(ref).split()
    # num_hyp = len(hypothesis_words)
    # num_ref = len(reference_words)
    p, r, f = prf(hypothesis_words, reference_words)
    # hyp_prefixes = [word[:PREFIX_LEN] for word in hypothesis_words]
    # ref_prefixes = [word[:PREFIX_LEN] for word in reference_words]
    # prefix_p, prefix_r, prefix_f = prf(hyp_prefixes, ref_prefixes)
    # hyp_stems = [cz_stem(word, aggressive=True) for word in hypothesis_words]
    # ref_stems = [cz_stem(word, aggressive=True) for word in reference_words]
    # stem_p, stem_r, stem_f = prf(hyp_stems, ref_stems)
    # aggro_lm_score = AGGRO_LM.score(' '.join(hyp_stems))
    # lm_score = LM.score(' '.join(hypothesis_words))
    results = {
        # 'token_precision': p,
        'token_recall':  r,
        # 'token_f1': f,
        # "stem_recall": stem_r,
        # "stem_precision": stem_p,
        # "stem_f1": stem_f,
        # 'prefix_precision': prefix_p,
        # 'prefix_recall': prefix_r,
        # 'prefix_f1': prefix_f,
        # 'morph_meteor': harmonic_mean(p, r, prefix_p, prefix_r),
        # 'stem_meteor': harmonic_mean(p, r, stem_p, stem_r),
        # 'len_diff': len(hypothesis_words) - len(reference_words),
        # 'len': .1 * num_hyp,
        # 'abs_len_diff': .2 * (abs(len(hypothesis_words) - len(reference_words))),
        # 'len_factor': num_hyp / float(num_ref),
        # 'len_factor^2':  0.5 * num_hyp**2 / float(num_ref**2),
        # 'aggro_lm_score': .02 * aggro_lm_score,
        # 'lm_score': .02 * lm_score,
        # 'len_normalized_aggro_lm_score': aggro_lm_score / num_hyp,
        # 'len_normalized_lm_score': lm_score / num_hyp,
    }
    # for n in range(3, 6):
    #     hyp_char_ngrams = [ngram for word in hypothesis_words for ngram in ngrams(word, n)]
    #     ref_char_ngrams = [ngram for word in reference_words for ngram in ngrams(word, n)]
    #     char_ngram_p, char_ngram_r, char_ngram_f = prf(hyp_char_ngrams, ref_char_ngrams)
    #     results.update({
    #         'char_%sgram_precision' % n: char_ngram_p,
    #         'char_%sgram_recall' % n: char_ngram_r,
    #         'char_%sgram_f1' % n: char_ngram_f,
    #     })
    # all_hyp_ngrams = list(hypothesis_words)
    # all_ref_ngrams = list(reference_words)
    # for n in range(2, 5):
    #     hyp_ngrams = ngrams(hypothesis_words, n)
    #     ref_ngrams = ngrams(reference_words, n)
    #     all_hyp_ngrams += hyp_ngrams
    #     all_ref_ngrams += ref_ngrams
    #     ngram_p, ngram_r, ngram_f = prf(hyp_ngrams, ref_ngrams)
    #     results.update({
    #         '%sgram_precision' % n: ngram_p,
    #         '%sgram_recall' % n: ngram_r,
    #         '%sgram_f1' % n: ngram_f,
    #     })
    # ngram_p, ngram_r, ngram_f = prf(all_hyp_ngrams, all_ref_ngrams)
    # results.update({
    #     'ngram_precision': ngram_p,
    #     'ngram_recall': ngram_r,
    #     'ngram_f1': ngram_f
    # })
    return results


if __name__ == "__main__":
    arg_parser = argparse.ArgumentParser(prog='extract')
    arg_parser.add_argument(
        '-x', '--pairs', dest='pairs', default='data/en-cs.pairs', help='Reference-Hypothesis pairs')
    args = arg_parser.parse_args()

    sys.stderr.write('Extracting features for (ref,hyp) pairs from %s.\n' % args.pairs)
    # loop over all (ref,hyp) pairs in the input file and extract evaluation features
    with open(args.pairs) as pairs_file:
        for i, line in enumerate(pairs_file):
            ref, hyp = line.rstrip().split(' ||| ')
            feature_map = extract_features(hyp, ref)
            if i % 1000 == 0:
                sys.stderr.write(".")
            print json.dumps(feature_map)   # print evaluation feature map
