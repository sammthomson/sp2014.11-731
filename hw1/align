#!/usr/bin/env python
import codecs
from collections import defaultdict
import optparse
import sys
from itertools import islice, product
from operator import itemgetter as ig


def read_sentence_pairs(bitext_filename, num_sentences):
    with codecs.open(bitext_filename, encoding="utf-8") as bitext_file:
        for pair in islice(bitext_file, num_sentences):
            yield tuple(sentence.strip().split() for sentence in pair.split(' ||| '))


def dice_coefficients(pair_counts, source_counts, target_counts, threshold):
    """
    Calculates the dice coefficients for all pairs of tokens
    Dice(A, B) := 2.0 * | A.intersection(B) | / (| A | + | B |
    """
    sys.stderr.write("calculating Dice similarities\n")
    dices = {}
    for (i, ((source_token, target_token), pair_count)) in enumerate(pair_counts.iteritems()):
        dice = 2.0 * pair_count / (source_counts[source_token] + target_counts[target_token])
        if dice >= threshold:
            dices[(source_token, target_token)] = dice
        if i % 5000 == 0:
            sys.stderr.write(".")
    sys.stderr.write("\n")
    return dices


def sentence_counts(bitext):
    sys.stderr.write("counting word/sentence co-occurrences.\n")
    # number of sentences each source word has been seen in
    source_counts = defaultdict(int)
    # number of sentences each target word has been seen in
    target_counts = defaultdict(int)
    # number of sentence pairs each (source, target) word  pair have been seen together in
    pair_counts = defaultdict(int)
    for (i, (source_sentence, target_sentence)) in enumerate(bitext):
        source_tokens = set(source_sentence)
        target_tokens = set(target_sentence)
        for source_token in source_tokens:
            source_counts[source_token] += 1
        for target_token in target_tokens:
            target_counts[target_token] += 1
        for (source_token, target_token) in product(source_tokens, target_tokens):
            pair_counts[(source_token, target_token)] += 1
        if i % 500 == 0:
            sys.stderr.write(".")
    sys.stderr.write("\n")
    return source_counts, target_counts, pair_counts


def align(bitext, threshold):
    for (source_sentence, target_sentence) in bitext:
        alignment = []
        for (i, source_token) in enumerate(source_sentence):
            target_candidates = [
                (j, dice.get((source_token, target_token), 0.0))
                for (j, target_token) in enumerate(target_sentence)
                if dice.get((source_token, target_token), 0.0) >= threshold
            ]
            if target_candidates:
                best_candidate = max(target_candidates, key=ig(1))[0]
                alignment.append((i, best_candidate))
        yield alignment


if __name__ == "__main__":
    opt_parser = optparse.OptionParser()
    opt_parser.add_option("-b", "--bitext", dest="bitext", default="data/dev-test-train.de-en",
                          help="Parallel corpus (default data/dev-test-train.de-en)")
    opt_parser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float",
                          help="Threshold for aligning with Dice's coefficient (default=0.5)")
    opt_parser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int",
                          help="Number of sentences to use for training and alignment")
    (opts, _) = opt_parser.parse_args()

    sys.stderr.write("Training with Dice's coefficient...\n")
    source_counts, target_counts, pair_counts = sentence_counts(read_sentence_pairs(opts.bitext, opts.num_sents))
    dice = dice_coefficients(pair_counts, source_counts, target_counts, opts.threshold)
    for alignment in align(read_sentence_pairs(opts.bitext, opts.num_sents), opts.threshold):
        sys.stdout.write(u" ".join("%i-%i" % (i, j) for (i, j) in alignment) + u"\n")
